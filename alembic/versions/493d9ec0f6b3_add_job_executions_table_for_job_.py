"""Add job_executions table for job monitoring

Revision ID: 493d9ec0f6b3
Revises: b74d972657e5
Create Date: 2025-11-11 12:01:16.648594

"""

from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = "493d9ec0f6b3"
down_revision: Union[str, Sequence[str], None] = "b74d972657e5"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "job_executions",
        sa.Column("id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column(
            "job_name",
            sa.String(length=255),
            nullable=False,
            comment="Celery Beat schedule name (e.g., 'ingest-slack-daily')",
        ),
        sa.Column(
            "job_category",
            sa.String(length=100),
            nullable=False,
            comment="Category: vector_ingestion, notifications, pm_automation, etc.",
        ),
        sa.Column(
            "task_id",
            sa.String(length=255),
            nullable=True,
            comment="Celery task ID (UUID) for correlation with Celery logs",
        ),
        sa.Column(
            "status",
            sa.String(length=50),
            nullable=False,
            comment="Status: running, success, failed, timeout, cancelled",
        ),
        sa.Column(
            "started_at",
            sa.TIMESTAMP(timezone=True),
            nullable=False,
            comment="When job execution began (UTC)",
        ),
        sa.Column(
            "completed_at",
            sa.TIMESTAMP(timezone=True),
            nullable=True,
            comment="When job execution finished (UTC)",
        ),
        sa.Column(
            "duration_seconds",
            sa.Integer(),
            nullable=True,
            comment="Execution duration in seconds (completed_at - started_at)",
        ),
        sa.Column(
            "result_data",
            postgresql.JSONB(astext_type=sa.Text()),
            nullable=True,
            comment="Task return value as JSON (e.g., {items_processed: 150, channels: 5})",
        ),
        sa.Column(
            "error_message",
            sa.Text(),
            nullable=True,
            comment="Error message if job failed",
        ),
        sa.Column(
            "error_traceback",
            sa.Text(),
            nullable=True,
            comment="Full Python stack trace for debugging",
        ),
        sa.Column(
            "retry_count",
            sa.Integer(),
            nullable=True,
            comment="Number of retries attempted by Celery",
        ),
        sa.Column(
            "worker_name",
            sa.String(length=255),
            nullable=True,
            comment="Celery worker hostname that executed this job",
        ),
        sa.Column(
            "celery_queue",
            sa.String(length=100),
            nullable=True,
            comment="Celery queue name (default, priority, etc.)",
        ),
        sa.Column(
            "priority",
            sa.String(length=50),
            nullable=True,
            comment="Job priority: low, normal, high, critical",
        ),
        sa.Column("created_at", sa.TIMESTAMP(timezone=True), nullable=False),
        sa.Column("updated_at", sa.TIMESTAMP(timezone=True), nullable=False),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(
        "idx_job_executions_category_status",
        "job_executions",
        ["job_category", "status"],
        unique=False,
    )
    op.create_index(
        "idx_job_executions_failures",
        "job_executions",
        ["job_name", "started_at"],
        unique=False,
        postgresql_where="status IN ('failed', 'timeout')",
    )
    op.create_index(
        "idx_job_executions_recent",
        "job_executions",
        ["started_at"],
        unique=False,
        postgresql_using="btree",
        postgresql_ops={"started_at": "DESC"},
    )
    op.create_index(
        op.f("ix_job_executions_job_category"),
        "job_executions",
        ["job_category"],
        unique=False,
    )
    op.create_index(
        op.f("ix_job_executions_job_name"), "job_executions", ["job_name"], unique=False
    )
    op.create_index(
        op.f("ix_job_executions_started_at"),
        "job_executions",
        ["started_at"],
        unique=False,
    )
    op.create_index(
        op.f("ix_job_executions_status"), "job_executions", ["status"], unique=False
    )
    op.create_index(
        op.f("ix_job_executions_task_id"), "job_executions", ["task_id"], unique=True
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column(
        "projects",
        sa.Column(
            "project_work_type",
            sa.VARCHAR(length=50),
            server_default=sa.text("'project-based'::character varying"),
            autoincrement=False,
            nullable=True,
        ),
    )
    op.add_column(
        "projects",
        sa.Column(
            "cumulative_hours",
            sa.NUMERIC(precision=10, scale=2),
            server_default=sa.text("0"),
            autoincrement=False,
            nullable=True,
        ),
    )
    op.add_column(
        "projects",
        sa.Column("start_date", sa.DATE(), autoincrement=False, nullable=True),
    )
    op.add_column(
        "projects",
        sa.Column(
            "retainer_hours",
            sa.NUMERIC(precision=10, scale=2),
            server_default=sa.text("0"),
            autoincrement=False,
            nullable=True,
        ),
    )
    op.add_column(
        "projects",
        sa.Column(
            "total_hours",
            sa.NUMERIC(precision=10, scale=2),
            server_default=sa.text("0"),
            autoincrement=False,
            nullable=True,
        ),
    )
    op.add_column(
        "projects",
        sa.Column("launch_date", sa.DATE(), autoincrement=False, nullable=True),
    )
    op.add_column(
        "projects",
        sa.Column(
            "send_meeting_emails",
            sa.BOOLEAN(),
            server_default=sa.text("false"),
            autoincrement=False,
            nullable=True,
        ),
    )
    op.add_column(
        "projects",
        sa.Column("description", sa.TEXT(), autoincrement=False, nullable=True),
    )
    op.add_column(
        "projects",
        sa.Column("weekly_meeting_day", sa.TEXT(), autoincrement=False, nullable=True),
    )
    op.create_index(
        op.f("idx_project_resource_mappings_key"),
        "project_resource_mappings",
        ["project_key"],
        unique=False,
    )
    op.create_index(
        op.f("ix_epic_hours_category"), "epic_hours", ["epic_category"], unique=False
    )
    op.create_table(
        "meeting_project_connections",
        sa.Column("id", sa.VARCHAR(), autoincrement=False, nullable=False),
        sa.Column("meeting_id", sa.VARCHAR(), autoincrement=False, nullable=False),
        sa.Column("meeting_title", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column(
            "meeting_date", postgresql.TIMESTAMP(), autoincrement=False, nullable=True
        ),
        sa.Column("project_key", sa.VARCHAR(), autoincrement=False, nullable=False),
        sa.Column("project_name", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("relevance_score", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("confidence", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column(
            "matching_factors",
            postgresql.JSON(astext_type=sa.Text()),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "created_at", postgresql.TIMESTAMP(), autoincrement=False, nullable=True
        ),
        sa.Column(
            "last_confirmed_at",
            postgresql.TIMESTAMP(),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column("is_verified", sa.BOOLEAN(), autoincrement=False, nullable=True),
        sa.PrimaryKeyConstraint("id", name=op.f("meeting_project_connections_pkey")),
    )
    op.create_table(
        "vector-sync-status",
        sa.Column("source", sa.TEXT(), autoincrement=False, nullable=False),
        sa.Column("last_sync", sa.TEXT(), autoincrement=False, nullable=False),
        sa.Column(
            "created_at",
            postgresql.TIMESTAMP(),
            server_default=sa.text("CURRENT_TIMESTAMP"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "updated_at",
            postgresql.TIMESTAMP(),
            server_default=sa.text("CURRENT_TIMESTAMP"),
            autoincrement=False,
            nullable=True,
        ),
        sa.PrimaryKeyConstraint("source", name=op.f("vector-sync-status_pkey")),
    )
    op.create_table(
        "project_changes",
        sa.Column("id", sa.VARCHAR(), autoincrement=False, nullable=False),
        sa.Column("project_key", sa.VARCHAR(), autoincrement=False, nullable=False),
        sa.Column("change_type", sa.VARCHAR(), autoincrement=False, nullable=False),
        sa.Column("ticket_key", sa.VARCHAR(), autoincrement=False, nullable=False),
        sa.Column("ticket_title", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("old_value", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("new_value", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("assignee", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("reporter", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("priority", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("status", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column(
            "change_timestamp",
            postgresql.TIMESTAMP(),
            autoincrement=False,
            nullable=False,
        ),
        sa.Column(
            "detected_at", postgresql.TIMESTAMP(), autoincrement=False, nullable=True
        ),
        sa.Column(
            "change_details",
            postgresql.JSON(astext_type=sa.Text()),
            autoincrement=False,
            nullable=True,
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("project_changes_pkey")),
    )
    op.create_table(
        "user_preferences",
        sa.Column("id", sa.VARCHAR(), autoincrement=False, nullable=False),
        sa.Column("email", sa.VARCHAR(), autoincrement=False, nullable=False),
        sa.Column("slack_username", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column(
            "notification_cadence", sa.VARCHAR(), autoincrement=False, nullable=True
        ),
        sa.Column(
            "selected_projects",
            postgresql.JSON(astext_type=sa.Text()),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "created_at", postgresql.TIMESTAMP(), autoincrement=False, nullable=True
        ),
        sa.Column(
            "updated_at", postgresql.TIMESTAMP(), autoincrement=False, nullable=True
        ),
        sa.Column(
            "last_notification_sent",
            postgresql.TIMESTAMP(),
            autoincrement=False,
            nullable=True,
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("user_preferences_pkey")),
        sa.UniqueConstraint(
            "email",
            name=op.f("user_preferences_email_key"),
            postgresql_include=[],
            postgresql_nulls_not_distinct=False,
        ),
    )
    op.drop_index(op.f("ix_job_executions_task_id"), table_name="job_executions")
    op.drop_index(op.f("ix_job_executions_status"), table_name="job_executions")
    op.drop_index(op.f("ix_job_executions_started_at"), table_name="job_executions")
    op.drop_index(op.f("ix_job_executions_job_name"), table_name="job_executions")
    op.drop_index(op.f("ix_job_executions_job_category"), table_name="job_executions")
    op.drop_index(
        "idx_job_executions_recent",
        table_name="job_executions",
        postgresql_using="btree",
        postgresql_ops={"started_at": "DESC"},
    )
    op.drop_index(
        "idx_job_executions_failures",
        table_name="job_executions",
        postgresql_where="status IN ('failed', 'timeout')",
    )
    op.drop_index("idx_job_executions_category_status", table_name="job_executions")
    op.drop_table("job_executions")
    # ### end Alembic commands ###
